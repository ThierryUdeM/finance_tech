name: Complete Trading Signal Pipeline

permissions:
  contents: read
  issues: write  # Required for creating issues

on:
  schedule:
    # Run every 15 minutes during market hours (9:30 AM - 4:00 PM ET, Monday-Friday)
    # GitHub Actions uses UTC time, so we need to adjust
    # 9:30 AM ET = 1:30 PM UTC (during EDT) or 2:30 PM UTC (during EST)
    - cron: '30,45 13-20 * * 1-5'  # Covers both EDT and EST periods
    - cron: '0,15 14-20 * * 1-5'
  
  workflow_dispatch:  # Allow manual trigger for testing

jobs:
  check-market-hours:
    runs-on: ubuntu-latest
    outputs:
      is_market_open: ${{ steps.check.outputs.is_open }}
    
    steps:
    - name: Check if market is open
      id: check
      run: |
        # Get current time in ET
        CURRENT_HOUR=$(TZ=America/New_York date +%H)
        CURRENT_MIN=$(TZ=America/New_York date +%M)
        CURRENT_DAY=$(TZ=America/New_York date +%u)
        
        # Check if weekend (6=Saturday, 7=Sunday)
        if [ $CURRENT_DAY -ge 6 ]; then
          echo "is_open=false" >> $GITHUB_OUTPUT
          echo "Market closed - Weekend"
          exit 0
        fi
        
        # Convert to minutes since midnight
        CURRENT_TOTAL_MIN=$((CURRENT_HOUR * 60 + CURRENT_MIN))
        MARKET_OPEN=$((9 * 60 + 30))   # 9:30 AM
        MARKET_CLOSE=$((16 * 60))      # 4:00 PM
        
        # Check market hours
        if [ $CURRENT_TOTAL_MIN -ge $MARKET_OPEN ] && [ $CURRENT_TOTAL_MIN -le $MARKET_CLOSE ]; then
          echo "is_open=true" >> $GITHUB_OUTPUT
          echo "Market is open"
        else
          echo "is_open=false" >> $GITHUB_OUTPUT
          echo "Market closed - Outside hours"
        fi

  generate-all-signals:
    needs: check-market-hours
    if: needs.check-market-hours.outputs.is_market_open == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Miniconda
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-update-conda: true
        python-version: '3.10'
        activate-environment: trading
        auto-activate-base: false
    
    - name: Install dependencies via conda
      shell: bash -l {0}
      run: |
        # Install TA-Lib and other dependencies via conda-forge
        conda install -c conda-forge ta-lib pandas numpy yfinance scipy scikit-learn -y
        
        # Install Azure dependencies via pip
        pip install azure-storage-blob python-dotenv
        
        # Verify TA-Lib installation
        python -c "import talib; print(f'TA-Lib version: {talib.__version__}')"
        python -c "import numpy as np; import talib; print(f'TA-Lib test: {talib.SMA(np.array([1.0,2.0,3.0,4.0,5.0]), 3)}')"
    
    - name: Run ensemble models for all tickers
      shell: bash -l {0}
      env:
        AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
        AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        AZURE_CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME }}
        STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
        ACCESS_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME }}
      run: |
        # Create a script to run models individually and save outputs
        cat > run_ensemble_models.py << 'EOF'
        import sys
        import os
        import json
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        import yfinance as yf
        from azure.storage.blob import BlobServiceClient
        import warnings
        warnings.filterwarnings('ignore')
        
        # Add ensemble directory to path
        ensemble_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'ensemble')
        sys.path.insert(0, ensemble_path)
        
        # Import ensemble models directly
        from ensemble_nvda import ensemble_nvda
        from ensemble_tsla import ensemble_tsla
        from ensemble_aapl_v2 import ensemble_aapl_v2
        from ensemble_msft_v2 import ensemble_msft_v2
        
        # Import market regime
        from market_regime import classify_regime
        
        # Define all tickers
        ALL_TICKERS = ['NVDA', 'TSLA', 'AAPL', 'MSFT']
        
        # Global cache for ticker data
        TICKER_DATA_CACHE = {}
        HISTORICAL_DATA_CACHE = {}
        
        # Print environment variables for debugging
        print("Azure Storage Account:", os.environ.get('AZURE_STORAGE_ACCOUNT', 'NOT SET'))
        print("Container Name:", os.environ.get('AZURE_CONTAINER_NAME', 'NOT SET'))
        
        def get_blob_service_client():
            """Initialize Azure Blob Service Client"""
            account_name = os.environ.get('AZURE_STORAGE_ACCOUNT') or os.environ.get('STORAGE_ACCOUNT_NAME')
            account_key = os.environ.get('AZURE_STORAGE_KEY') or os.environ.get('ACCESS_KEY')
            
            if not account_name or not account_key:
                raise ValueError("Azure storage credentials not found in environment variables")
            
            connection_string = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net"
            return BlobServiceClient.from_connection_string(connection_string)
        
        def load_historical_data_from_azure(ticker):
            """Load 2.5 years of historical data from Azure databento folder"""
            if ticker in HISTORICAL_DATA_CACHE:
                return HISTORICAL_DATA_CACHE[ticker]
            
            try:
                blob_service_client = get_blob_service_client()
                container_name = os.environ.get('AZURE_CONTAINER_NAME') or os.environ.get('CONTAINER_NAME')
                
                blob_path = f'databento/{ticker}_historical_data.json'
                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
                
                # Download the blob
                blob_data = blob_client.download_blob().readall()
                data = json.loads(blob_data)
                
                # Convert to DataFrame
                df = pd.DataFrame(data)
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df.set_index('timestamp', inplace=True)
                
                # Ensure column names are lowercase
                df.columns = [col.lower() for col in df.columns]
                
                # Calculate returns and vwap if not present
                if 'returns' not in df.columns:
                    df['returns'] = df['close'].pct_change()
                if 'vwap' not in df.columns:
                    df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()
                
                HISTORICAL_DATA_CACHE[ticker] = df
                print(f"Loaded {len(df)} historical records for {ticker} from Azure")
                return df
                
            except Exception as e:
                print(f"Error loading historical data for {ticker} from Azure: {str(e)}")
                return None
        
        def fetch_all_ticker_data(lookback_days=1):
            """Fetch current data for all tickers individually with delays"""
            global TICKER_DATA_CACHE
            import time
            
            print("Fetching current data for tickers individually...")
            
            # Fetch tickers one by one with delays to avoid rate limiting
            for i, ticker in enumerate(ALL_TICKERS):
                if i > 0:
                    print(f"Waiting 2 seconds before fetching {ticker}...")
                    time.sleep(2)  # Delay between requests
                
                try:
                    fetch_data(ticker, lookback_days)
                except Exception as e:
                    print(f"Error fetching {ticker}: {str(e)}")
            
            return  # Exit early - we're using individual fetching
        
        def fetch_data(ticker, lookback_days=1):
            """Fetch current data for model predictions"""
            global TICKER_DATA_CACHE
            import time
            
            # Check cache first
            if ticker in TICKER_DATA_CACHE:
                print(f"Using cached data for {ticker}")
                return TICKER_DATA_CACHE[ticker]
            
            # Add retry logic for rate limiting
            max_retries = 3
            retry_delay = 5  # seconds
            
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        print(f"Retry attempt {attempt + 1} for {ticker}")
                        time.sleep(retry_delay * attempt)  # Exponential backoff
                    
                    # Fetch only 1 day of current data for predictions
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=lookback_days)
                    
                    data = yf.download(ticker, start=start_date, end=end_date, 
                                     interval='15m', progress=False)
                    
                    if data.empty:
                        return None
                    
                    data.columns = [col.lower() for col in data.columns]
                    data.reset_index(inplace=True)
                    data.rename(columns={'Datetime': 'timestamp'}, inplace=True)
                    data['returns'] = data['close'].pct_change()
                    data['vwap'] = (data['close'] * data['volume']).cumsum() / data['volume'].cumsum()
                    
                    # Cache the data
                    TICKER_DATA_CACHE[ticker] = data
                    return data
                except Exception as e:
                    if "Rate limited" in str(e) and attempt < max_retries - 1:
                        continue
                    print(f"Error fetching {ticker}: {str(e)}")
                    return None
        
        def run_model_for_ticker(ticker, model_func):
            """Run ensemble model and extract detailed outputs"""
            print(f"Running model for {ticker}...")
            
            # Load historical data from Azure for pattern matching
            historical_data = load_historical_data_from_azure(ticker)
            if historical_data is None:
                print(f"No historical data available for {ticker} in Azure")
                return None
            
            # Fetch current data from Yahoo
            current_data = fetch_data(ticker)
            if current_data is None:
                print(f"No current data available for {ticker} from Yahoo")
                return None
                
            print(f"Historical data shape for {ticker}: {historical_data.shape}")
            print(f"Current data shape for {ticker}: {current_data.shape}")
            
            # Use ALL 2.5 years of historical data as the pattern library
            # The momentum model will search through this entire history
            train_data = historical_data.copy()
            
            # Use current data for predictions (last few hours)
            test_data = current_data.iloc[-26:].copy()  # Last 6.5 hours of current data
            
            try:
                # Run model
                signals = model_func(train_data, test_data)
                
                # Get latest values
                if 'signal' in signals.columns:
                    signal = int(signals['signal'].iloc[-1])
                else:
                    signal = int(signals.iloc[-1, 0])
                
                position_size = signals['position_size'].iloc[-1] if 'position_size' in signals.columns else 1.0
                
                # Calculate regime
                regime_data = classify_regime(test_data)
                
                # Extract model internals if available
                model_details = {
                    'ticker': ticker,
                    'timestamp': datetime.now().isoformat(),
                    'signal': signal,
                    'position_size': float(position_size),
                    'latest_price': float(test_data['close'].iloc[-1]),
                    'vwap': float(test_data['vwap'].iloc[-1]),
                    'volume_ratio': float(test_data['volume'].iloc[-1] / test_data['volume'].mean()),
                    'regime': {
                        'label': regime_data['regime_label'].iloc[-1],
                        'score': float(regime_data['regime_score'].iloc[-1]),
                        'adx': float(regime_data['adx'].iloc[-1]),
                        'hurst': float(regime_data['hurst'].iloc[-1])
                    },
                    'data_points': len(test_data),
                    'model_type': 'V1' if ticker in ['NVDA', 'TSLA'] else 'V2'
                }
                
                return model_details
                
            except Exception as e:
                print(f"Error running model for {ticker}: {str(e)}")
                return None
        
        # Model configurations
        models = {
            'NVDA': ensemble_nvda,
            'TSLA': ensemble_tsla,
            'AAPL': ensemble_aapl_v2,
            'MSFT': ensemble_msft_v2
        }
        
        # Fetch current data for all tickers with rate limiting protection
        fetch_all_ticker_data()
        
        # Run all models
        all_model_outputs = []
        
        for ticker, model_func in models.items():
            output = run_model_for_ticker(ticker, model_func)
            if output:
                all_model_outputs.append(output)
        
        # Save model outputs
        model_output = {
            'generated_at': datetime.now().isoformat(),
            'models_run': len(all_model_outputs),
            'model_outputs': all_model_outputs
        }
        
        with open('ensemble_model_outputs.json', 'w') as f:
            json.dump(model_output, f, indent=2)
        
        print(f"\nModel outputs saved. Total models run: {len(all_model_outputs)}")
        EOF
        
        python run_ensemble_models.py
    
    - name: Generate trading signals
      shell: bash -l {0}
      run: |
        cd ensemble
        python generate_trading_signals.py
        cd ..
    
    - name: Generate position sizing report
      shell: bash -l {0}
      run: |
        cd ensemble
        python position_sizing_calculator.py
        cd ..
    
    - name: Upload all outputs to Azure
      shell: bash -l {0}
      env:
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
      run: |
        # Create comprehensive upload script
        cat > upload_all_outputs.py << 'EOF'
        import os
        import json
        from azure.storage.blob import BlobServiceClient
        from datetime import datetime
        
        # Get Azure credentials
        account_name = os.environ.get('STORAGE_ACCOUNT_NAME')
        container_name = os.environ.get('CONTAINER_NAME')
        access_key = os.environ.get('ACCESS_KEY')
        
        if not all([account_name, container_name, access_key]):
            print("Error: Azure credentials not set")
            exit(1)
        
        # Create connection string from components
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={access_key};EndpointSuffix=core.windows.net"
        
        # Create blob service client
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        
        # Create combined output
        timestamp = datetime.now()
        
        # Load all outputs
        outputs = {}
        
        # Model outputs
        if os.path.exists('ensemble_model_outputs.json'):
            with open('ensemble_model_outputs.json', 'r') as f:
                outputs['model_outputs'] = json.load(f)
        
        # Trading signals
        if os.path.exists('ensemble/trading_signals_latest.json'):
            with open('ensemble/trading_signals_latest.json', 'r') as f:
                outputs['trading_signals'] = json.load(f)
        
        # Signal summary
        if os.path.exists('ensemble/actionable_summary_latest.json'):
            with open('ensemble/actionable_summary_latest.json', 'r') as f:
                outputs['signal_summary'] = json.load(f)
        
        # Position sizing
        if os.path.exists('ensemble/position_sizing_report.json'):
            with open('ensemble/position_sizing_report.json', 'r') as f:
                outputs['position_sizing'] = json.load(f)
        
        # Create master output file
        master_output = {
            'pipeline_run_time': timestamp.isoformat(),
            'outputs': outputs
        }
        
        # Files to upload
        files_to_upload = [
            ('ensemble_model_outputs.json', 'model_outputs/latest.json'),
            ('ensemble/trading_signals_latest.json', 'signals/latest.json'),
            ('ensemble/actionable_summary_latest.json', 'summary/latest.json'),
            ('ensemble/position_sizing_report.json', 'sizing/latest.json')
        ]
        
        # Upload individual files
        for local_file, blob_name in files_to_upload:
            if os.path.exists(local_file):
                # Upload latest version
                blob_client = blob_service_client.get_blob_client(
                    container=container_name, 
                    blob=blob_name
                )
                
                with open(local_file, 'rb') as data:
                    blob_client.upload_blob(data, overwrite=True)
                    print(f"Uploaded {blob_name}")
                
                # Also save timestamped version
                timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
                history_blob = blob_name.replace('latest.json', f'{timestamp_str}.json')
                history_blob = f"history/{history_blob}"
                
                blob_client_history = blob_service_client.get_blob_client(
                    container=container_name,
                    blob=history_blob
                )
                
                with open(local_file, 'rb') as data:
                    blob_client_history.upload_blob(data, overwrite=True)
                    print(f"Uploaded {history_blob}")
        
        # Save and upload master output
        with open('master_output.json', 'w') as f:
            json.dump(master_output, f, indent=2)
        
        # Upload master output
        master_blob = blob_service_client.get_blob_client(
            container=container_name,
            blob='master/latest.json'
        )
        
        with open('master_output.json', 'rb') as data:
            master_blob.upload_blob(data, overwrite=True)
            print("Uploaded master output")
        
        # Also save timestamped master
        master_history = f"history/master/{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        master_history_blob = blob_service_client.get_blob_client(
            container=container_name,
            blob=master_history
        )
        
        with open('master_output.json', 'rb') as data:
            master_history_blob.upload_blob(data, overwrite=True)
            print(f"Uploaded {master_history}")
        
        print("\nAll uploads complete!")
        print(f"Total files uploaded: {len(files_to_upload) * 2 + 2}")
        EOF
        
        python upload_all_outputs.py
    
    - name: Create summary report
      if: github.event_name == 'workflow_dispatch'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## ðŸ“Š Trading Signal Pipeline Complete!\n\n';
          comment += `**Run Time:** ${new Date().toISOString()}\n\n`;
          
          // Read model outputs
          if (fs.existsSync('ensemble_model_outputs.json')) {
            const modelOutputs = JSON.parse(fs.readFileSync('ensemble_model_outputs.json', 'utf8'));
            comment += '### Model Outputs\n';
            comment += `Models Run: ${modelOutputs.models_run}\n\n`;
            
            modelOutputs.model_outputs.forEach(m => {
              const signal = m.signal === 1 ? 'ðŸŸ¢ BUY' : m.signal === -1 ? 'ðŸ”´ SELL' : 'âšª NEUTRAL';
              comment += `**${m.ticker}**: ${signal} (Confidence: ${(m.position_size * 100).toFixed(1)}%)\n`;
              comment += `- Price: $${m.latest_price.toFixed(2)} | VWAP: $${m.vwap.toFixed(2)}\n`;
              comment += `- Regime: ${m.regime.label} (${m.regime.score.toFixed(2)})\n\n`;
            });
          }
          
          // Read signal summary
          if (fs.existsSync('ensemble/actionable_summary_latest.json')) {
            const summary = JSON.parse(fs.readFileSync('ensemble/actionable_summary_latest.json', 'utf8'));
            
            comment += '### Signal Summary\n';
            comment += `- Total Signals: ${summary.total_signals}\n`;
            comment += `- Buy Signals: ${summary.buy_signals}\n`;
            comment += `- Sell Signals: ${summary.sell_signals}\n`;
            comment += `- High Confidence: ${summary.high_confidence}\n\n`;
            
            if (summary.top_opportunities.length > 0) {
              comment += '### Top Opportunities\n\n';
              summary.top_opportunities.forEach((opp, i) => {
                comment += `${i + 1}. **${opp.ticker}** - ${opp.action}\n`;
                comment += `   - Entry: ${opp.entry_range} | Stop: ${opp.stop}\n`;
                comment += `   - Target (1h): ${opp.target_1h} (RR: ${opp.risk_reward_1h})\n\n`;
              });
            }
          }
          
          comment += '\n### Azure Storage Structure\n';
          comment += '```\n';
          comment += 'trading-signals/\n';
          comment += 'â”œâ”€â”€ master/latest.json          # Combined output\n';
          comment += 'â”œâ”€â”€ model_outputs/latest.json   # Raw model outputs\n';
          comment += 'â”œâ”€â”€ signals/latest.json         # Trading signals\n';
          comment += 'â”œâ”€â”€ summary/latest.json         # Actionable summary\n';
          comment += 'â””â”€â”€ sizing/latest.json          # Position sizing\n';
          comment += '```';
          
          // Create issue if manually triggered
          if (context.eventName === 'workflow_dispatch') {
            try {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Trading Signals - ${new Date().toLocaleDateString()}`,
                body: comment,
                labels: ['trading-signals', 'automated']
              });
              console.log('Issue created successfully');
            } catch (error) {
              console.log('Note: Could not create issue. To enable issue creation:');
              console.log('1. Go to Settings > Actions > General');
              console.log('2. Under "Workflow permissions", select "Read and write permissions"');
              console.log('3. Or add issues: write permission to the workflow file');
              console.log('\nSummary Report:\n');
              console.log(comment);
            }
          }