name: Complete Trading Signal Pipeline

permissions:
  contents: read
  issues: write  # Required for creating issues

on:
  schedule:
    # Run every 15 minutes during market hours (9:30 AM - 4:00 PM ET, Monday-Friday)
    # GitHub Actions uses UTC time, so we need to adjust
    # 9:30 AM ET = 1:30 PM UTC (during EDT) or 2:30 PM UTC (during EST)
    - cron: '30,45 13-20 * * 1-5'  # Covers both EDT and EST periods
    - cron: '0,15 14-20 * * 1-5'
  
  workflow_dispatch:  # Allow manual trigger for testing

jobs:
  check-market-hours:
    runs-on: ubuntu-latest
    outputs:
      is_market_open: ${{ steps.check.outputs.is_open }}
    
    steps:
    - name: Check if market is open
      id: check
      run: |
        # Get current time in ET
        CURRENT_HOUR=$(TZ=America/New_York date +%H)
        CURRENT_MIN=$(TZ=America/New_York date +%M)
        CURRENT_DAY=$(TZ=America/New_York date +%u)
        
        # Check if weekend (6=Saturday, 7=Sunday)
        if [ $CURRENT_DAY -ge 6 ]; then
          echo "is_open=false" >> $GITHUB_OUTPUT
          echo "Market closed - Weekend"
          exit 0
        fi
        
        # Convert to minutes since midnight
        CURRENT_TOTAL_MIN=$((CURRENT_HOUR * 60 + CURRENT_MIN))
        MARKET_OPEN=$((9 * 60 + 30))   # 9:30 AM
        MARKET_CLOSE=$((16 * 60))      # 4:00 PM
        
        # Check market hours
        if [ $CURRENT_TOTAL_MIN -ge $MARKET_OPEN ] && [ $CURRENT_TOTAL_MIN -le $MARKET_CLOSE ]; then
          echo "is_open=true" >> $GITHUB_OUTPUT
          echo "Market is open"
        else
          echo "is_open=false" >> $GITHUB_OUTPUT
          echo "Market closed - Outside hours"
        fi

  generate-all-signals:
    needs: check-market-hours
    if: needs.check-market-hours.outputs.is_market_open == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Miniconda
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-update-conda: true
        python-version: '3.10'
        activate-environment: trading
        auto-activate-base: false
    
    - name: Install dependencies via conda
      shell: bash -l {0}
      run: |
        # Install TA-Lib and other dependencies via conda-forge
        conda install -c conda-forge ta-lib pandas numpy scipy scikit-learn -y
        
        # Install/upgrade yfinance and Azure dependencies via pip
        pip install --upgrade --no-cache-dir yfinance
        pip install azure-storage-blob python-dotenv
        
        # Show yfinance version
        python -c "import yfinance as yf; print(f'yfinance version: {yf.__version__}')"
        
        # Verify TA-Lib installation
        python -c "import talib; print(f'TA-Lib version: {talib.__version__}')"
        python -c "import numpy as np; import talib; print(f'TA-Lib test: {talib.SMA(np.array([1.0,2.0,3.0,4.0,5.0]), 3)}')"
    
    - name: Run ensemble models for all tickers
      shell: bash -l {0}
      env:
        AZURE_STORAGE_ACCOUNT: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        AZURE_STORAGE_KEY: ${{ secrets.ACCESS_KEY }}
        AZURE_CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        # Create a script to run models individually and save outputs
        cat > run_ensemble_models.py << 'EOF'
        import sys
        import os
        import json
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        import yfinance as yf
        from azure.storage.blob import BlobServiceClient
        import warnings
        warnings.filterwarnings('ignore')
        
        # Add ensemble directory to path
        ensemble_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'ensemble')
        sys.path.insert(0, ensemble_path)
        
        # Print yfinance version for debugging
        print(f"yfinance version: {yf.__version__}")
        
        # Import ensemble models directly
        from ensemble_nvda import ensemble_nvda
        from ensemble_tsla import ensemble_tsla
        from ensemble_aapl_v2 import ensemble_aapl_v2
        from ensemble_msft_v2 import ensemble_msft_v2
        
        # Import market regime
        from market_regime import classify_regime
        
        # Define all tickers
        ALL_TICKERS = ['NVDA', 'TSLA', 'AAPL', 'MSFT']
        
        # Global cache for ticker data
        TICKER_DATA_CACHE = {}
        HISTORICAL_DATA_CACHE = {}
        
        # Print environment variables for debugging
        azure_account = os.environ.get('AZURE_STORAGE_ACCOUNT', '')
        azure_container = os.environ.get('AZURE_CONTAINER_NAME', '')
        print(f"Azure Storage Account: {'SET' if azure_account else 'NOT SET'}")
        print(f"Container Name: {'SET' if azure_container else 'NOT SET'}")
        
        if not azure_account or not azure_container:
            print("WARNING: Azure credentials are not set. The workflow will continue without historical data.")
            print("To use Azure historical data, set these GitHub secrets:")
            print("  - STORAGE_ACCOUNT_NAME")
            print("  - ACCESS_KEY") 
            print("  - CONTAINER_NAME")
        
        def get_blob_service_client():
            """Initialize Azure Blob Service Client"""
            account_name = os.environ.get('AZURE_STORAGE_ACCOUNT') or os.environ.get('STORAGE_ACCOUNT_NAME')
            account_key = os.environ.get('AZURE_STORAGE_KEY') or os.environ.get('ACCESS_KEY')
            
            if not account_name or not account_key:
                raise ValueError("Azure storage credentials not found in environment variables")
            
            connection_string = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net"
            return BlobServiceClient.from_connection_string(connection_string)
        
        def load_historical_data_from_azure(ticker):
            """Load 2.5 years of historical data from Azure databento folder"""
            if ticker in HISTORICAL_DATA_CACHE:
                return HISTORICAL_DATA_CACHE[ticker]
            
            try:
                blob_service_client = get_blob_service_client()
                container_name = os.environ.get('AZURE_CONTAINER_NAME') or os.environ.get('CONTAINER_NAME')
                
                blob_path = f'databento/{ticker}_historical_data.json'
                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
                
                # Download the blob
                blob_data = blob_client.download_blob().readall()
                data = json.loads(blob_data)
                
                print(f"Azure data type for {ticker}: {type(data)}")
                
                # Handle different data structures
                if isinstance(data, dict):
                    print(f"Dict keys: {list(data.keys())}")
                    # Check if data is nested in a 'data' key
                    if 'data' in data and isinstance(data['data'], list):
                        print(f"Found 'data' key with {len(data['data'])} records")
                        df = pd.DataFrame(data['data'])
                    else:
                        # Try to create DataFrame from dict directly
                        df = pd.DataFrame(data)
                elif isinstance(data, list):
                    print(f"List with {len(data)} records")
                    df = pd.DataFrame(data)
                else:
                    raise ValueError(f"Unexpected data type: {type(data)}")
                
                # Ensure column names are lowercase
                df.columns = [col.lower() for col in df.columns]
                
                # Handle timestamp column
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df.set_index('timestamp', inplace=True)
                elif 'datetime' in df.columns:
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    df.set_index('datetime', inplace=True)
                    
                # Ensure we have the required columns
                required_cols = ['open', 'high', 'low', 'close', 'volume']
                missing_cols = [col for col in required_cols if col not in df.columns]
                if missing_cols:
                    print(f"Missing required columns for {ticker}: {missing_cols}")
                    return None
                
                # Calculate returns and vwap if not present
                if 'returns' not in df.columns:
                    df['returns'] = df['close'].pct_change()
                if 'vwap' not in df.columns:
                    df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()
                
                HISTORICAL_DATA_CACHE[ticker] = df
                print(f"Loaded {len(df)} historical records for {ticker} from Azure")
                return df
                
            except Exception as e:
                print(f"Error loading historical data for {ticker} from Azure: {str(e)}")
                return None
        
        def check_azure_cache(filename):
            """Check if today's data is already cached in Azure"""
            try:
                blob_service_client = get_blob_service_client()
                container_name = os.environ.get('AZURE_CONTAINER_NAME') or os.environ.get('CONTAINER_NAME')
                blob_path = f'cache/{filename}'
                
                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
                return blob_client.exists()
            except:
                return False
        
        def load_cached_data(filename):
            """Load cached data from Azure"""
            try:
                blob_service_client = get_blob_service_client()
                container_name = os.environ.get('AZURE_CONTAINER_NAME') or os.environ.get('CONTAINER_NAME')
                blob_path = f'cache/{filename}'
                
                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
                
                # Download to temp file
                temp_file = f'/tmp/{filename}'
                with open(temp_file, 'wb') as f:
                    f.write(blob_client.download_blob().readall())
                
                # Load parquet file
                import pandas as pd
                data = pd.read_parquet(temp_file)
                
                # Process each ticker
                for ticker in ALL_TICKERS:
                    if ticker in data.columns.levels[0]:
                        ticker_data = data[ticker].copy()
                        ticker_data = ticker_data.reset_index()
                        ticker_data.rename(columns={'Date': 'timestamp', 'Datetime': 'timestamp'}, inplace=True)
                        ticker_data.columns = [col.lower() for col in ticker_data.columns]
                        
                        if 'returns' not in ticker_data.columns:
                            ticker_data['returns'] = ticker_data['close'].pct_change()
                        if 'vwap' not in ticker_data.columns:
                            ticker_data['vwap'] = (ticker_data['close'] * ticker_data['volume']).cumsum() / ticker_data['volume'].cumsum()
                        
                        TICKER_DATA_CACHE[ticker] = ticker_data
                        print(f"Loaded cached data for {ticker}: {len(ticker_data)} rows")
                
                os.remove(temp_file)
                return True
            except Exception as e:
                print(f"Error loading cache: {str(e)}")
                return False
        
        def save_to_azure_cache(filename, data):
            """Save data to Azure cache"""
            try:
                blob_service_client = get_blob_service_client()
                container_name = os.environ.get('AZURE_CONTAINER_NAME') or os.environ.get('CONTAINER_NAME')
                blob_path = f'cache/{filename}'
                
                # Save to temp file first
                temp_file = f'/tmp/{filename}'
                data.to_parquet(temp_file)
                
                # Upload to Azure
                blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
                with open(temp_file, 'rb') as f:
                    blob_client.upload_blob(f, overwrite=True)
                
                os.remove(temp_file)
                print(f"Saved cache to Azure: {blob_path}")
            except Exception as e:
                print(f"Error saving cache: {str(e)}")
        
        def fetch_all_ticker_data(lookback_days=1):
            """Fetch current data for all tickers in one shot"""
            global TICKER_DATA_CACHE
            import time
            
            print("Fetching current data for all tickers in bulk...")
            
            # Check if we have cached data for today
            cache_file = f"live_data_{datetime.now().strftime('%Y%m%d')}.parquet"
            
            # Try to use cached data first
            if check_azure_cache(cache_file):
                print(f"Using cached data from {cache_file}")
                load_cached_data(cache_file)
                return
            
            # Fetch all tickers in one request
            max_retries = 3
            retry_delays = [15, 45, 120]  # Exponential backoff: 15s, 45s, 2min
            
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        delay = retry_delays[attempt - 1]
                        print(f"Retry attempt {attempt + 1} - waiting {delay} seconds...")
                        time.sleep(delay)
                    
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=lookback_days)
                    
                    # Download all tickers at once
                    print(f"Downloading data for: {', '.join(ALL_TICKERS)}")
                    data = yf.download(
                        tickers=ALL_TICKERS,
                        start=start_date,
                        end=end_date,
                        interval='15m',
                        group_by='ticker',
                        threads=False,  # Single thread as recommended
                        progress=False
                    )
                    
                    if data.empty:
                        raise Exception("No data returned from yfinance")
                    
                    # Process each ticker's data
                    for ticker in ALL_TICKERS:
                        try:
                            if len(ALL_TICKERS) == 1:
                                ticker_data = data
                            else:
                                ticker_data = data[ticker]
                            
                            # Clean and prepare data
                            ticker_data = ticker_data.copy()
                            ticker_data = ticker_data.reset_index()
                            ticker_data.rename(columns={'Date': 'timestamp', 'Datetime': 'timestamp'}, inplace=True)
                            ticker_data.columns = [col.lower() for col in ticker_data.columns]
                            ticker_data['returns'] = ticker_data['close'].pct_change()
                            ticker_data['vwap'] = (ticker_data['close'] * ticker_data['volume']).cumsum() / ticker_data['volume'].cumsum()
                            
                            TICKER_DATA_CACHE[ticker] = ticker_data
                            print(f"Cached data for {ticker}: {len(ticker_data)} rows")
                        except Exception as e:
                            print(f"Error processing {ticker}: {str(e)}")
                    
                    # Save to cache if successful
                    if len(TICKER_DATA_CACHE) > 0:
                        save_to_azure_cache(cache_file, data)
                    
                    return  # Success!
                    
                except Exception as e:
                    print(f"Error in bulk download (attempt {attempt + 1}): {str(e)}")
                    if attempt == max_retries - 1:
                        print("All retries exhausted. Giving up.")
                    
            return
        
        def get_cached_data(ticker):
            """Get data for a ticker from cache"""
            if ticker in TICKER_DATA_CACHE:
                return TICKER_DATA_CACHE[ticker]
            return None
        
        def run_model_for_ticker(ticker, model_func):
            """Run ensemble model and extract detailed outputs"""
            print(f"Running model for {ticker}...")
            
            # Load historical data from Azure for pattern matching
            historical_data = load_historical_data_from_azure(ticker)
            if historical_data is None:
                print(f"No historical data available for {ticker} in Azure")
                return None
            
            # Get current data from cache
            current_data = get_cached_data(ticker)
            if current_data is None:
                print(f"No current data available for {ticker}")
                return None
                
            print(f"Historical data shape for {ticker}: {historical_data.shape}")
            print(f"Current data shape for {ticker}: {current_data.shape}")
            
            # Use ALL 2.5 years of historical data as the pattern library
            # The momentum model will search through this entire history
            train_data = historical_data.copy()
            
            # Use current data for predictions (last few hours)
            test_data = current_data.iloc[-26:].copy()  # Last 6.5 hours of current data
            
            try:
                # Run model
                signals = model_func(train_data, test_data)
                
                # Get latest values
                if 'signal' in signals.columns:
                    signal = int(signals['signal'].iloc[-1])
                else:
                    signal = int(signals.iloc[-1, 0])
                
                position_size = signals['position_size'].iloc[-1] if 'position_size' in signals.columns else 1.0
                
                # Calculate regime
                regime_data = classify_regime(test_data)
                
                # Extract model internals if available
                model_details = {
                    'ticker': ticker,
                    'timestamp': datetime.now().isoformat(),
                    'signal': signal,
                    'position_size': float(position_size),
                    'latest_price': float(test_data['close'].iloc[-1]),
                    'vwap': float(test_data['vwap'].iloc[-1]),
                    'volume_ratio': float(test_data['volume'].iloc[-1] / test_data['volume'].mean()),
                    'regime': {
                        'label': regime_data['regime_label'].iloc[-1],
                        'score': float(regime_data['regime_score'].iloc[-1]),
                        'adx': float(regime_data['adx'].iloc[-1]),
                        'hurst': float(regime_data['hurst'].iloc[-1])
                    },
                    'data_points': len(test_data),
                    'model_type': 'V1' if ticker in ['NVDA', 'TSLA'] else 'V2'
                }
                
                return model_details
                
            except Exception as e:
                print(f"Error running model for {ticker}: {str(e)}")
                return None
        
        # Model configurations
        models = {
            'NVDA': ensemble_nvda,
            'TSLA': ensemble_tsla,
            'AAPL': ensemble_aapl_v2,
            'MSFT': ensemble_msft_v2
        }
        
        # Fetch current data for all tickers with rate limiting protection
        fetch_all_ticker_data()
        
        # Run all models
        all_model_outputs = []
        
        for ticker, model_func in models.items():
            output = run_model_for_ticker(ticker, model_func)
            if output:
                all_model_outputs.append(output)
        
        # Save model outputs
        model_output = {
            'generated_at': datetime.now().isoformat(),
            'models_run': len(all_model_outputs),
            'model_outputs': all_model_outputs
        }
        
        with open('ensemble_model_outputs.json', 'w') as f:
            json.dump(model_output, f, indent=2)
        
        print(f"\nModel outputs saved. Total models run: {len(all_model_outputs)}")
        EOF
        
        python run_ensemble_models.py
    
    - name: Generate trading signals
      shell: bash -l {0}
      run: |
        cd ensemble
        python generate_trading_signals.py
        cd ..
    
    - name: Generate position sizing report
      shell: bash -l {0}
      run: |
        cd ensemble
        python position_sizing_calculator.py
        cd ..
    
    - name: Upload all outputs to Azure
      shell: bash -l {0}
      env:
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
      run: |
        # Create comprehensive upload script
        cat > upload_all_outputs.py << 'EOF'
        import os
        import json
        from azure.storage.blob import BlobServiceClient
        from datetime import datetime
        
        # Get Azure credentials
        account_name = os.environ.get('STORAGE_ACCOUNT_NAME')
        container_name = os.environ.get('CONTAINER_NAME')
        access_key = os.environ.get('ACCESS_KEY')
        
        if not all([account_name, container_name, access_key]):
            print("Error: Azure credentials not set")
            exit(1)
        
        # Create connection string from components
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={access_key};EndpointSuffix=core.windows.net"
        
        # Create blob service client
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        
        # Create combined output
        timestamp = datetime.now()
        
        # Load all outputs
        outputs = {}
        
        # Model outputs
        if os.path.exists('ensemble_model_outputs.json'):
            with open('ensemble_model_outputs.json', 'r') as f:
                outputs['model_outputs'] = json.load(f)
        
        # Trading signals
        if os.path.exists('ensemble/trading_signals_latest.json'):
            with open('ensemble/trading_signals_latest.json', 'r') as f:
                outputs['trading_signals'] = json.load(f)
        
        # Signal summary
        if os.path.exists('ensemble/actionable_summary_latest.json'):
            with open('ensemble/actionable_summary_latest.json', 'r') as f:
                outputs['signal_summary'] = json.load(f)
        
        # Position sizing
        if os.path.exists('ensemble/position_sizing_report.json'):
            with open('ensemble/position_sizing_report.json', 'r') as f:
                outputs['position_sizing'] = json.load(f)
        
        # Create master output file
        master_output = {
            'pipeline_run_time': timestamp.isoformat(),
            'outputs': outputs
        }
        
        # Files to upload
        files_to_upload = [
            ('ensemble_model_outputs.json', 'model_outputs/latest.json'),
            ('ensemble/trading_signals_latest.json', 'signals/latest.json'),
            ('ensemble/actionable_summary_latest.json', 'summary/latest.json'),
            ('ensemble/position_sizing_report.json', 'sizing/latest.json')
        ]
        
        # Upload individual files
        for local_file, blob_name in files_to_upload:
            if os.path.exists(local_file):
                # Upload latest version
                blob_client = blob_service_client.get_blob_client(
                    container=container_name, 
                    blob=blob_name
                )
                
                with open(local_file, 'rb') as data:
                    blob_client.upload_blob(data, overwrite=True)
                    print(f"Uploaded {blob_name}")
                
                # Also save timestamped version
                timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
                history_blob = blob_name.replace('latest.json', f'{timestamp_str}.json')
                history_blob = f"history/{history_blob}"
                
                blob_client_history = blob_service_client.get_blob_client(
                    container=container_name,
                    blob=history_blob
                )
                
                with open(local_file, 'rb') as data:
                    blob_client_history.upload_blob(data, overwrite=True)
                    print(f"Uploaded {history_blob}")
        
        # Save and upload master output
        with open('master_output.json', 'w') as f:
            json.dump(master_output, f, indent=2)
        
        # Upload master output
        master_blob = blob_service_client.get_blob_client(
            container=container_name,
            blob='master/latest.json'
        )
        
        with open('master_output.json', 'rb') as data:
            master_blob.upload_blob(data, overwrite=True)
            print("Uploaded master output")
        
        # Also save timestamped master
        master_history = f"history/master/{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        master_history_blob = blob_service_client.get_blob_client(
            container=container_name,
            blob=master_history
        )
        
        with open('master_output.json', 'rb') as data:
            master_history_blob.upload_blob(data, overwrite=True)
            print(f"Uploaded {master_history}")
        
        print("\nAll uploads complete!")
        print(f"Total files uploaded: {len(files_to_upload) * 2 + 2}")
        EOF
        
        python upload_all_outputs.py
    
    - name: Create summary report
      if: github.event_name == 'workflow_dispatch'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## ðŸ“Š Trading Signal Pipeline Complete!\n\n';
          comment += `**Run Time:** ${new Date().toISOString()}\n\n`;
          
          // Read model outputs
          if (fs.existsSync('ensemble_model_outputs.json')) {
            const modelOutputs = JSON.parse(fs.readFileSync('ensemble_model_outputs.json', 'utf8'));
            comment += '### Model Outputs\n';
            comment += `Models Run: ${modelOutputs.models_run}\n\n`;
            
            modelOutputs.model_outputs.forEach(m => {
              const signal = m.signal === 1 ? 'ðŸŸ¢ BUY' : m.signal === -1 ? 'ðŸ”´ SELL' : 'âšª NEUTRAL';
              comment += `**${m.ticker}**: ${signal} (Confidence: ${(m.position_size * 100).toFixed(1)}%)\n`;
              comment += `- Price: $${m.latest_price.toFixed(2)} | VWAP: $${m.vwap.toFixed(2)}\n`;
              comment += `- Regime: ${m.regime.label} (${m.regime.score.toFixed(2)})\n\n`;
            });
          }
          
          // Read signal summary
          if (fs.existsSync('ensemble/actionable_summary_latest.json')) {
            const summary = JSON.parse(fs.readFileSync('ensemble/actionable_summary_latest.json', 'utf8'));
            
            comment += '### Signal Summary\n';
            comment += `- Total Signals: ${summary.total_signals}\n`;
            comment += `- Buy Signals: ${summary.buy_signals}\n`;
            comment += `- Sell Signals: ${summary.sell_signals}\n`;
            comment += `- High Confidence: ${summary.high_confidence}\n\n`;
            
            if (summary.top_opportunities.length > 0) {
              comment += '### Top Opportunities\n\n';
              summary.top_opportunities.forEach((opp, i) => {
                comment += `${i + 1}. **${opp.ticker}** - ${opp.action}\n`;
                comment += `   - Entry: ${opp.entry_range} | Stop: ${opp.stop}\n`;
                comment += `   - Target (1h): ${opp.target_1h} (RR: ${opp.risk_reward_1h})\n\n`;
              });
            }
          }
          
          comment += '\n### Azure Storage Structure\n';
          comment += '```\n';
          comment += 'trading-signals/\n';
          comment += 'â”œâ”€â”€ master/latest.json          # Combined output\n';
          comment += 'â”œâ”€â”€ model_outputs/latest.json   # Raw model outputs\n';
          comment += 'â”œâ”€â”€ signals/latest.json         # Trading signals\n';
          comment += 'â”œâ”€â”€ summary/latest.json         # Actionable summary\n';
          comment += 'â””â”€â”€ sizing/latest.json          # Position sizing\n';
          comment += '```';
          
          // Create issue if manually triggered
          if (context.eventName === 'workflow_dispatch') {
            try {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Trading Signals - ${new Date().toLocaleDateString()}`,
                body: comment,
                labels: ['trading-signals', 'automated']
              });
              console.log('Issue created successfully');
            } catch (error) {
              console.log('Note: Could not create issue. To enable issue creation:');
              console.log('1. Go to Settings > Actions > General');
              console.log('2. Under "Workflow permissions", select "Read and write permissions"');
              console.log('3. Or add issues: write permission to the workflow file');
              console.log('\nSummary Report:\n');
              console.log(comment);
            }
          }