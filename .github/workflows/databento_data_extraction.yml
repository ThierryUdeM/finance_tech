name: Databento Historical Data Extraction

on:
  workflow_dispatch:
    inputs:
      ticker:
        description: 'Ticker to extract data for (e.g., AAPL, MSFT, BTC-USD)'
        required: true
        type: string
      start_date:
        description: 'Start date (YYYY-MM-DD, default: 2.5 years ago)'
        required: false
        type: string
      end_date:
        description: 'End date (YYYY-MM-DD, default: today)'
        required: false
        type: string
      overwrite_existing:
        description: 'Overwrite existing data file'
        required: false
        default: false
        type: boolean

env:
  PYTHONPATH: ${{ github.workspace }}
  TZ: America/New_York

jobs:
  extract-databento-data:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install databento and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install databento pandas numpy pytz
        pip install azure-storage-blob
        
    - name: Validate ticker input
      run: |
        TICKER="${{ github.event.inputs.ticker }}"
        echo "Extracting data for ticker: $TICKER"
        
        # Validate ticker format
        if [[ ! "$TICKER" =~ ^[A-Z0-9.-]+$ ]]; then
          echo "Error: Invalid ticker format. Use uppercase letters, numbers, dots, and hyphens only."
          exit 1
        fi
        
        # Set default dates if not provided
        if [ -z "${{ github.event.inputs.start_date }}" ]; then
          # Default to 2.5 years ago (using Python for cross-platform compatibility)
          START_DATE=$(python -c "import datetime; print((datetime.datetime.now() - datetime.timedelta(days=912)).strftime('%Y-%m-%d'))")
        else
          START_DATE="${{ github.event.inputs.start_date }}"
        fi
        
        if [ -z "${{ github.event.inputs.end_date }}" ]; then
          END_DATE=$(python -c "import datetime; print(datetime.datetime.now().strftime('%Y-%m-%d'))")
        else
          END_DATE="${{ github.event.inputs.end_date }}"
        fi
        
        echo "TICKER=$TICKER" >> $GITHUB_ENV
        echo "START_DATE=$START_DATE" >> $GITHUB_ENV
        echo "END_DATE=$END_DATE" >> $GITHUB_ENV
        
        echo "Data extraction parameters:"
        echo "  Ticker: $TICKER"
        echo "  Start Date: $START_DATE"
        echo "  End Date: $END_DATE"
        
    - name: Check if data already exists
      id: check_data
      run: |
        DATA_FILE="data/${TICKER}_15min_pattern_ready.csv"
        if [ -f "$DATA_FILE" ] && [ "${{ github.event.inputs.overwrite_existing }}" != "true" ]; then
          echo "Data file already exists: $DATA_FILE"
          echo "Use overwrite_existing=true to replace it"
          echo "should_extract=false" >> $GITHUB_OUTPUT
        else
          echo "should_extract=true" >> $GITHUB_OUTPUT
        fi
        
    - name: Extract historical data from Databento
      if: steps.check_data.outputs.should_extract == 'true'
      env:
        DATABENTO_API_KEY: ${{ secrets.DATABENTO_API_KEY }}
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        python << 'EOF'
        import os
        import pandas as pd
        import databento as db
        from datetime import datetime, timedelta
        import pytz
        # Environment variables are already set by GitHub Actions
        
        # Configuration
        TICKER = os.environ['TICKER']
        START_DATE = os.environ['START_DATE']
        END_DATE = os.environ['END_DATE']
        
        print(f"Starting Databento extraction for {TICKER}")
        print(f"Date range: {START_DATE} to {END_DATE}")
        
        # Initialize Databento client
        client = db.Historical(key=os.environ['DATABENTO_API_KEY'])
        
        try:
            # Map common tickers to Databento symbols if needed
            symbol_mapping = {
                'BTC-USD': 'BTCUSD',  # Try without hyphen for crypto
                'AAPL': 'AAPL',
                'MSFT': 'MSFT', 
                'GOOGL': 'GOOGL',
                'AMZN': 'AMZN',
                'TSLA': 'TSLA',
                'AC.TO': 'AC',  # Toronto Stock Exchange symbols may need adjustment
                'SPY': 'SPY',
                'QQQ': 'QQQ'
            }
            
            # Try multiple symbol variations for crypto
            if TICKER == 'BTC-USD':
                symbols_to_try = ['BTCUSD', 'BTC-USD', 'BTC.USD', 'XBTUSD']
                datasets_to_try = ['GLBX.MDP3', 'DBEQ.BASIC']
            else:
                databento_symbol = symbol_mapping.get(TICKER, TICKER)
                symbols_to_try = [databento_symbol]
                
                # Determine dataset based on ticker
                if TICKER.endswith('.TO'):
                    datasets_to_try = ['XTOR.MDP3']  # Toronto Stock Exchange
                else:
                    datasets_to_try = ['XNAS.ITCH', 'DBEQ.BASIC']  # NASDAQ for most US stocks
            
            print(f"Will try symbols: {symbols_to_try}")
            print(f"Will try datasets: {datasets_to_try}")
            
            # Try different combinations
            df = None
            for dataset in datasets_to_try:
                for symbol in symbols_to_try:
                    try:
                        print(f"Trying {dataset} with symbol {symbol}...")
                        data = client.timeseries.get_range(
                            dataset=dataset,
                            symbols=[symbol],
                            schema='ohlcv-1m',  # 1-minute OHLCV bars
                            start=START_DATE,
                            end=END_DATE,
                            limit=200000
                        )
                        
                        # Convert to DataFrame first
                        df = data.to_df()
                        
                        if not df.empty:
                            print(f"✅ Success with {dataset} and symbol {symbol}")
                            print(f"Got {len(df)} records")
                            break
                        else:
                            print(f"No data for {dataset} with {symbol}")
                            
                    except Exception as e:
                        print(f"Failed {dataset} with {symbol}: {str(e)}")
                        continue
                
                if df is not None and not df.empty:
                    break
            
            if df is None or df.empty:
                print(f"No data returned for {TICKER} after trying all combinations.")
                print("Check symbol mapping, dataset availability, and API access.")
                exit(1)
            
            # Ensure we have the required columns
            required_columns = ['ts_event', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"Missing required columns: {missing_columns}")
                print(f"Available columns: {list(df.columns)}")
                exit(1)
            
            # Resample 1-minute data to 15-minute intervals
            if 'ts_event' in df.columns:
                df['timestamp'] = pd.to_datetime(df['ts_event'])
                df.set_index('timestamp', inplace=True)
                
                # Resample to 15-minute intervals
                df_resampled = df.resample('15T').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min', 
                    'close': 'last',
                    'volume': 'sum'
                }).dropna()
                
                # Reset index and use resampled data
                df_resampled.reset_index(inplace=True)
                df = df_resampled
                df['ts_event'] = df['timestamp']
            
            # Rename and format columns to match NVDA data structure
            df_formatted = pd.DataFrame({
                'timestamp': pd.to_datetime(df['ts_event']),
                'Open': df['open'].astype(float),
                'High': df['high'].astype(float), 
                'Low': df['low'].astype(float),
                'Close': df['close'].astype(float),
                'Volume': df['volume'].astype(int)
            })
            
            # Filter to regular trading hours (9:30 AM - 4:00 PM ET)
            et_tz = pytz.timezone('US/Eastern')
            df_formatted['timestamp'] = df_formatted['timestamp'].dt.tz_convert(et_tz)
            
            # Filter for regular market hours
            market_hours = (
                (df_formatted['timestamp'].dt.hour > 9) |
                ((df_formatted['timestamp'].dt.hour == 9) & (df_formatted['timestamp'].dt.minute >= 30))
            ) & (df_formatted['timestamp'].dt.hour < 16)
            
            # Filter for weekdays only
            weekdays = df_formatted['timestamp'].dt.weekday < 5
            
            df_filtered = df_formatted[market_hours & weekdays].copy()
            
            # Sort by timestamp
            df_filtered = df_filtered.sort_values('timestamp').reset_index(drop=True)
            
            print(f"Data extraction summary:")
            print(f"  Total records: {len(df_filtered):,}")
            print(f"  Date range: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}")
            print(f"  Unique trading days: {df_filtered['timestamp'].dt.date.nunique()}")
            
            # Create data directory if it doesn't exist
            os.makedirs('data', exist_ok=True)
            
            # Save to CSV
            output_file = f'data/{TICKER}_15min_pattern_ready.csv'
            df_filtered.to_csv(output_file, index=False)
            
            print(f"Successfully saved data to {output_file}")
            
            # Validate minimum data requirements
            min_records_needed = 2000  # Approximately 2-3 months of trading data
            if len(df_filtered) < min_records_needed:
                print(f"WARNING: Only {len(df_filtered)} records extracted.")
                print(f"Signal model works best with {min_records_needed}+ records for robust pattern matching.")
            else:
                print(f"✅ Sufficient data extracted ({len(df_filtered)} records) for signal model")
                
        except Exception as e:
            print(f"Error during data extraction: {str(e)}")
            print(f"This could be due to:")
            print(f"  - Invalid ticker symbol for Databento")
            print(f"  - Incorrect dataset selection")
            print(f"  - API key issues")
            print(f"  - Date range issues")
            exit(1)
        
        EOF
        
    - name: Upload data to Azure (optional backup)
      if: steps.check_data.outputs.should_extract == 'true'
      continue-on-error: true
      env:
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        python << 'EOF'
        import os
        from azure.storage.blob import BlobServiceClient
        # Environment variables are already set by GitHub Actions
        
        try:
            # Initialize Azure client
            account_name = os.environ['STORAGE_ACCOUNT_NAME']
            access_key = os.environ['ACCESS_KEY']
            container_name = os.environ['CONTAINER_NAME']
            
            blob_service_client = BlobServiceClient(
                account_url=f"https://{account_name}.blob.core.windows.net",
                credential=access_key
            )
            
            # Upload data file
            ticker = os.environ['TICKER']
            local_file = f'data/{ticker}_15min_pattern_ready.csv'
            blob_path = f'historical_data/{ticker}_15min_pattern_ready.csv'
            
            with open(local_file, 'rb') as data:
                blob_service_client.get_blob_client(
                    container=container_name,
                    blob=blob_path
                ).upload_blob(data, overwrite=True)
            
            print(f"✅ Uploaded {local_file} to Azure as {blob_path}")
            
        except Exception as e:
            print(f"Azure upload failed (non-critical): {str(e)}")
        
        EOF
        
    - name: Commit data file to repository
      if: steps.check_data.outputs.should_extract == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        DATA_FILE="data/${TICKER}_15min_pattern_ready.csv"
        
        if [ -f "$DATA_FILE" ]; then
          git add "$DATA_FILE"
          git commit -m "Add Databento historical data for ${TICKER}

          📊 Data extracted from Databento API
          📅 Date range: ${START_DATE} to ${END_DATE}  
          🕐 15-minute intervals during market hours
          
          🤖 Generated with Claude Code
          
          Co-Authored-By: Claude <noreply@anthropic.com>"
          
          git push
          echo "✅ Data file committed to repository"
        else
          echo "❌ Data file not found, commit skipped"
        fi
        
    - name: Generate data summary report
      if: steps.check_data.outputs.should_extract == 'true'
      run: |
        python << 'EOF'
        import pandas as pd
        import os
        
        ticker = os.environ['TICKER']
        data_file = f'data/{ticker}_15min_pattern_ready.csv'
        
        if os.path.exists(data_file):
            df = pd.read_csv(data_file)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            print(f"\n📊 Data Summary Report for {ticker}")
            print("=" * 50)
            print(f"Total Records: {len(df):,}")
            print(f"Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}")
            print(f"Trading Days: {df['timestamp'].dt.date.nunique()}")
            print(f"Avg Daily Records: {len(df) / df['timestamp'].dt.date.nunique():.1f}")
            print(f"Price Range: ${df['Low'].min():.2f} - ${df['High'].max():.2f}")
            print(f"Avg Daily Volume: {df['Volume'].mean():,.0f}")
            print("\n✅ Data ready for signal model pattern matching")
            
            # Check data quality
            missing_data = df.isnull().sum().sum()
            if missing_data > 0:
                print(f"⚠️  Warning: {missing_data} missing data points detected")
            else:
                print("✅ No missing data points")
                
        EOF
        
    - name: Upload extraction logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: databento-extraction-${{ env.TICKER }}-${{ github.run_number }}
        path: |
          data/${{ env.TICKER }}_15min_pattern_ready.csv
          *.log
        retention-days: 30