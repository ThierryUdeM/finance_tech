name: Databento Historical Data Extraction

on:
  workflow_dispatch:
    inputs:
      ticker:
        description: 'Ticker to extract data for (e.g., AAPL, MSFT, BTC-USD)'
        required: true
        type: string
      start_date:
        description: 'Start date (YYYY-MM-DD, default: 2.5 years ago)'
        required: false
        type: string
      end_date:
        description: 'End date (YYYY-MM-DD, default: today)'
        required: false
        type: string
      overwrite_existing:
        description: 'Overwrite existing data file'
        required: false
        default: false
        type: boolean

env:
  PYTHONPATH: ${{ github.workspace }}
  TZ: America/New_York

jobs:
  extract-databento-data:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install databento and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install databento pandas numpy pytz
        pip install azure-storage-blob
        
    - name: Validate ticker input
      run: |
        TICKER="${{ github.event.inputs.ticker }}"
        echo "Extracting data for ticker: $TICKER"
        
        # Validate ticker format
        if [[ ! "$TICKER" =~ ^[A-Z0-9.-]+$ ]]; then
          echo "Error: Invalid ticker format. Use uppercase letters, numbers, dots, and hyphens only."
          exit 1
        fi
        
        # Set default dates if not provided
        if [ -z "${{ github.event.inputs.start_date }}" ]; then
          # Default to 2.5 years ago (using Python for cross-platform compatibility)
          START_DATE=$(python -c "import datetime; print((datetime.datetime.now() - datetime.timedelta(days=912)).strftime('%Y-%m-%d'))")
        else
          START_DATE="${{ github.event.inputs.start_date }}"
        fi
        
        if [ -z "${{ github.event.inputs.end_date }}" ]; then
          END_DATE=$(python -c "import datetime; print(datetime.datetime.now().strftime('%Y-%m-%d'))")
        else
          END_DATE="${{ github.event.inputs.end_date }}"
        fi
        
        echo "TICKER=$TICKER" >> $GITHUB_ENV
        echo "START_DATE=$START_DATE" >> $GITHUB_ENV
        echo "END_DATE=$END_DATE" >> $GITHUB_ENV
        
        echo "Data extraction parameters:"
        echo "  Ticker: $TICKER"
        echo "  Start Date: $START_DATE"
        echo "  End Date: $END_DATE"
        
    - name: Check if data already exists in Azure
      id: check_data
      env:
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        if [ "${{ github.event.inputs.overwrite_existing }}" == "true" ]; then
          echo "Overwrite requested, will extract new data"
          echo "should_extract=true" >> $GITHUB_OUTPUT
        else
          python << 'EOF'
        import os
        import sys
        from azure.storage.blob import BlobServiceClient
        from datetime import datetime, timedelta
        
        # Configuration
        TICKER = os.environ['TICKER']
        account_name = os.environ['STORAGE_ACCOUNT_NAME']
        access_key = os.environ['ACCESS_KEY']
        container_name = os.environ['CONTAINER_NAME']
        
        # Initialize Azure client
        blob_service_client = BlobServiceClient(
            account_url=f"https://{account_name}.blob.core.windows.net",
            credential=access_key
        )
        
        # Check if data exists
        blob_path = f'databento/{TICKER}_historical_data.json'
        blob_client = blob_service_client.get_blob_client(
            container=container_name,
            blob=blob_path
        )
        
        try:
            # Check if blob exists and when it was last modified
            properties = blob_client.get_blob_properties()
            last_modified = properties.last_modified
            
            # Check if data is recent (updated within last 24 hours)
            if datetime.utcnow().replace(tzinfo=last_modified.tzinfo) - last_modified < timedelta(hours=24):
                print(f"Recent data exists for {TICKER} (updated {last_modified})")
                print("Use overwrite_existing=true to force update")
                print("should_extract=false", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
            else:
                print(f"Data exists but is stale (last updated {last_modified})")
                print("Will update with new data")
                print("should_extract=true", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
        except:
            print(f"No existing data found for {TICKER}")
            print("should_extract=true", file=open(os.environ['GITHUB_OUTPUT'], 'a'))
        EOF
        fi
        
    - name: Extract historical data from Databento
      if: steps.check_data.outputs.should_extract == 'true'
      env:
        DATABENTO_API_KEY: ${{ secrets.DATABENTO_API_KEY }}
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        python << 'EOF'
        import os
        import pandas as pd
        import databento as db
        from datetime import datetime, timedelta
        import pytz
        
        # Configuration
        TICKER = os.environ['TICKER']
        START_DATE = os.environ['START_DATE']
        END_DATE = os.environ['END_DATE']
        
        print(f"Starting Databento extraction for {TICKER}")
        print(f"Date range: {START_DATE} to {END_DATE}")
        
        # Initialize Databento client
        client = db.Historical(key=os.environ['DATABENTO_API_KEY'])
        
        try:
            # Try multiple symbol variations for crypto
            if TICKER == 'BTC-USD':
                symbols_to_try = ['BTCUSD', 'BTC-USD', 'BTC.USD', 'XBTUSD']
                datasets_to_try = ['GLBX.MDP3', 'DBEQ.BASIC']
            else:
                # Map other tickers
                symbol_mapping = {
                    'AAPL': 'AAPL',
                    'MSFT': 'MSFT', 
                    'GOOGL': 'GOOGL',
                    'AMZN': 'AMZN',
                    'TSLA': 'TSLA',
                    'AC.TO': 'AC',
                    'SPY': 'SPY',
                    'QQQ': 'QQQ'
                }
                databento_symbol = symbol_mapping.get(TICKER, TICKER)
                symbols_to_try = [databento_symbol]
                
                # Determine dataset based on ticker
                if TICKER.endswith('.TO'):
                    datasets_to_try = ['XTOR.MDP3']
                else:
                    datasets_to_try = ['XNAS.ITCH', 'DBEQ.BASIC']
            
            print(f"Will try symbols: {symbols_to_try}")
            print(f"Will try datasets: {datasets_to_try}")
            
            # Try different combinations
            df = None
            for dataset in datasets_to_try:
                for symbol in symbols_to_try:
                    try:
                        print(f"Trying {dataset} with symbol {symbol}...")
                        
                        # Get data from Databento
                        bars = client.timeseries.get_range(
                            dataset=dataset,
                            symbols=[symbol],
                            schema='ohlcv-1m',
                            start=START_DATE,
                            end=END_DATE,
                            limit=200000
                        )
                        
                        # CRITICAL: Convert to DataFrame BEFORE checking empty
                        df = bars.to_df()
                        
                        if not df.empty:
                            print(f"✅ Success with {dataset} and symbol {symbol}")
                            print(f"Got {len(df)} records")
                            break
                        else:
                            print(f"No data for {dataset} with {symbol}")
                            
                    except Exception as e:
                        print(f"Failed {dataset} with {symbol}: {str(e)}")
                        continue
                
                if df is not None and not df.empty:
                    break
            
            # Check if we got any data
            if df is None or df.empty:
                print(f"No data returned for {TICKER} after trying all combinations.")
                print("This could be due to:")
                print("  - Invalid ticker symbol for Databento")
                print("  - Incorrect dataset selection")
                print("  - API key issues")
                print("  - Date range issues")
                exit(1)
            
            # Check available columns and adapt
            print(f"Available columns: {list(df.columns)}")
            print(f"Index name: {df.index.name}")
            print(f"Index type: {type(df.index)}")
            
            # Check if the index is already a DatetimeIndex (common for OHLCV data)
            if isinstance(df.index, pd.DatetimeIndex):
                print("Timestamp is already in the index")
                # Just rename the index to 'timestamp' for consistency
                df.index.name = 'timestamp'
            else:
                # Find the timestamp column
                timestamp_col = None
                if 'ts_event' in df.columns:
                    timestamp_col = 'ts_event'
                elif 'ts_recv' in df.columns:
                    timestamp_col = 'ts_recv'
                else:
                    # Try to find any column with 'time' in the name
                    for col in df.columns:
                        if 'time' in col.lower() or 'ts' in col.lower():
                            timestamp_col = col
                            break
                
                if timestamp_col:
                    print(f"Using timestamp column: {timestamp_col}")
                    df['timestamp'] = pd.to_datetime(df[timestamp_col])
                    df.set_index('timestamp', inplace=True)
                else:
                    # If no timestamp column found, the index might be numeric (row numbers)
                    # This might mean the data doesn't have timestamps, which would be unusual
                    print("WARNING: No timestamp found in columns or index")
                    print(f"First few rows of data:\n{df.head()}")
                    exit(1)
            
            # Ensure we have OHLCV columns
            required_ohlcv = ['open', 'high', 'low', 'close', 'volume']
            missing_ohlcv = [col for col in required_ohlcv if col not in df.columns]
            if missing_ohlcv:
                print(f"Missing required OHLCV columns: {missing_ohlcv}")
                exit(1)
            
            # Resample 1-minute data to 15-minute intervals
            print("Resampling 1-minute data to 15-minute intervals...")
            
            # Resample to 15-minute intervals
            df_resampled = df.resample('15T').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min', 
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            # Reset index and use resampled data
            df_resampled.reset_index(inplace=True)
            df = df_resampled
            
            # Rename and format columns to match expected structure
            df_formatted = pd.DataFrame({
                'timestamp': df['timestamp'],
                'Open': df['open'].astype(float),
                'High': df['high'].astype(float), 
                'Low': df['low'].astype(float),
                'Close': df['close'].astype(float),
                'Volume': df['volume'].astype(int)
            })
            
            # Filter to regular trading hours (9:30 AM - 4:00 PM ET)
            et_tz = pytz.timezone('US/Eastern')
            
            # Handle timezone conversion
            if df_formatted['timestamp'].dt.tz is None:
                # If no timezone, assume UTC and convert to ET
                df_formatted['timestamp'] = df_formatted['timestamp'].dt.tz_localize('UTC').dt.tz_convert(et_tz)
            else:
                # If already has timezone, just convert to ET
                df_formatted['timestamp'] = df_formatted['timestamp'].dt.tz_convert(et_tz)
            
            # Filter for regular market hours (skip for crypto)
            if TICKER != 'BTC-USD':
                market_hours = (
                    (df_formatted['timestamp'].dt.hour > 9) |
                    ((df_formatted['timestamp'].dt.hour == 9) & (df_formatted['timestamp'].dt.minute >= 30))
                ) & (df_formatted['timestamp'].dt.hour < 16)
                
                # Filter for weekdays only
                weekdays = df_formatted['timestamp'].dt.weekday < 5
                df_filtered = df_formatted[market_hours & weekdays].copy()
            else:
                # For crypto, keep all hours
                df_filtered = df_formatted.copy()
            
            # Sort by timestamp
            df_filtered = df_filtered.sort_values('timestamp').reset_index(drop=True)
            
            print(f"Data extraction summary:")
            print(f"  Total records: {len(df_filtered):,}")
            print(f"  Date range: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}")
            print(f"  Unique trading days: {df_filtered['timestamp'].dt.date.nunique()}")
            
            # Create data directory if it doesn't exist
            os.makedirs('data', exist_ok=True)
            
            # Save to CSV
            output_file = f'data/{TICKER}_15min_pattern_ready.csv'
            df_filtered.to_csv(output_file, index=False)
            
            print(f"Successfully saved data to {output_file}")
            
            # Validate minimum data requirements
            min_records_needed = 1000
            if len(df_filtered) < min_records_needed:
                print(f"WARNING: Only {len(df_filtered)} records extracted.")
                print(f"Signal model works best with {min_records_needed}+ records.")
            else:
                print(f"✅ Sufficient data extracted ({len(df_filtered)} records)")
                
        except Exception as e:
            print(f"Error during data extraction: {str(e)}")
            print("This could be due to:")
            print("  - Invalid ticker symbol for Databento")
            print("  - Incorrect dataset selection") 
            print("  - API key issues")
            print("  - Date range issues")
            exit(1)
        
        EOF
        
    - name: Upload data to Azure Storage
      if: steps.check_data.outputs.should_extract == 'true'
      env:
        STORAGE_ACCOUNT_NAME: ${{ secrets.STORAGE_ACCOUNT_NAME }}
        ACCESS_KEY: ${{ secrets.ACCESS_KEY }}
        CONTAINER_NAME: ${{ secrets.CONTAINER_NAME }}
      run: |
        python << 'EOF'
        import os
        import json
        import pandas as pd
        from datetime import datetime
        from azure.storage.blob import BlobServiceClient
        
        # Configuration
        TICKER = os.environ['TICKER']
        account_name = os.environ['STORAGE_ACCOUNT_NAME']
        access_key = os.environ['ACCESS_KEY']
        container_name = os.environ['CONTAINER_NAME']
        
        # Read the CSV file
        csv_file = f'data/{TICKER}_15min_pattern_ready.csv'
        if not os.path.exists(csv_file):
            print(f"❌ CSV file not found: {csv_file}")
            exit(1)
            
        df = pd.read_csv(csv_file)
        print(f"Loading {len(df)} records for {TICKER}")
        
        # Initialize Azure client
        blob_service_client = BlobServiceClient(
            account_url=f"https://{account_name}.blob.core.windows.net",
            credential=access_key
        )
        
        # Create container client
        container_client = blob_service_client.get_container_client(container_name)
        
        # Define blob path
        blob_path = f'databento/{TICKER}_historical_data.json'
        
        try:
            # Try to download existing data
            blob_client = container_client.get_blob_client(blob_path)
            existing_data = blob_client.download_blob().readall()
            existing_records = json.loads(existing_data)
            print(f"Found existing data with {len(existing_records)} records")
            
            # Convert existing records to DataFrame for merging
            existing_df = pd.DataFrame(existing_records)
            existing_df['timestamp'] = pd.to_datetime(existing_df['timestamp'])
            
            # Convert new data timestamp
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Merge data (new data takes precedence for overlapping timestamps)
            merged_df = pd.concat([existing_df, df]).drop_duplicates(
                subset=['timestamp'], 
                keep='last'
            ).sort_values('timestamp')
            
            print(f"Merged data has {len(merged_df)} total records")
            
        except Exception as e:
            print(f"No existing data found or error reading: {str(e)}")
            print("Creating new dataset")
            merged_df = df
            merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'])
            merged_df = merged_df.sort_values('timestamp')
        
        # Convert to JSON format
        records = merged_df.to_dict('records')
        
        # Convert timestamps to ISO format for JSON serialization
        for record in records:
            record['timestamp'] = record['timestamp'].isoformat()
        
        # Create metadata
        metadata = {
            'ticker': TICKER,
            'total_records': len(records),
            'date_range': {
                'start': records[0]['timestamp'],
                'end': records[-1]['timestamp']
            },
            'last_updated': datetime.utcnow().isoformat(),
            'source': 'databento',
            'schema': 'ohlcv-15m'
        }
        
        # Combine metadata and data
        final_data = {
            'metadata': metadata,
            'data': records
        }
        
        # Upload to Azure
        json_data = json.dumps(final_data, indent=2)
        blob_client = container_client.get_blob_client(blob_path)
        blob_client.upload_blob(json_data, overwrite=True)
        
        print(f"✅ Successfully uploaded {TICKER} data to Azure")
        print(f"   Blob path: {blob_path}")
        print(f"   Total records: {len(records)}")
        print(f"   Date range: {metadata['date_range']['start']} to {metadata['date_range']['end']}")
        
        # Also save a summary file
        summary_path = f'databento/summary.json'
        try:
            # Get existing summary
            summary_blob = container_client.get_blob_client(summary_path)
            summary_data = json.loads(summary_blob.download_blob().readall())
        except:
            summary_data = {'tickers': {}}
        
        # Update summary
        summary_data['tickers'][TICKER] = {
            'last_updated': metadata['last_updated'],
            'total_records': metadata['total_records'],
            'date_range': metadata['date_range']
        }
        summary_data['last_updated'] = datetime.utcnow().isoformat()
        
        # Upload summary
        summary_blob = container_client.get_blob_client(summary_path)
        summary_blob.upload_blob(json.dumps(summary_data, indent=2), overwrite=True)
        
        print(f"✅ Updated summary file")
        
        # Clean up local CSV file (optional)
        # os.remove(csv_file)
        # print(f"🧹 Removed local CSV file")
        
        EOF